{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请求线程1号\n",
      "请求线程2号\n",
      "请求线程3号\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread 解析线程1号:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\naive\\Anaconda3\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "TypeError: run() takes 0 positional arguments but 1 was given\n",
      "\n",
      "Exception in thread 解析线程2号:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\naive\\Anaconda3\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "TypeError: run() takes 0 positional arguments but 1 was given\n",
      "\n",
      "Exception in thread 解析线程3号:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\naive\\Anaconda3\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "TypeError: run() takes 0 positional arguments but 1 was given\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head><title>404 Not Found</title></head>\n",
      "<body bgcolor=\"white\">\n",
      "<center><h1>404 Not Found</h1></center>\n",
      "<hr><center>nginx</center>\n",
      "</body>\n",
      "</html>\n",
      "<html>\n",
      "<head><title>404 Not Found</title></head>\n",
      "<body bgcolor=\"white\">\n",
      "<center><h1>404 Not Found</h1></center>\n",
      "<hr><center>nginx</center>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "\n",
      "<html>\n",
      "<head><title>404 Not Found</title></head>\n",
      "<body bgcolor=\"white\">\n",
      "<center><h1>404 Not Found</h1></center>\n",
      "<hr><center>nginx</center>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "<html>\n",
      "<head><title>404 Not Found</title></head>\n",
      "<body bgcolor=\"white\">\n",
      "<center><h1>404 Not Found</h1></center>\n",
      "<hr><center>nginx</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#用来存放请求线程\n",
    "g_crawl_list=[]\n",
    "#用来存放解析线程\n",
    "g_parse_list=[]\n",
    "def create_queue():\n",
    "    #先创建页码队列，使得可以多线程请求url\n",
    "    page_queue=Queue()\n",
    "    for i in range(1,5):\n",
    "        page_queue.put(i)\n",
    "     \n",
    "    #创建数据队列，使得获得的数据可以多线程进行解析\n",
    "    data_queue=Queue()\n",
    "    return page_queue,data_queue\n",
    "\n",
    "#我们通过这个函数来创建请求线程\n",
    "def create_crawl_thread(page_queue,data_queue):\n",
    "    #通过创建crawl_name来方便我们了解程序在运行哪个线程\n",
    "    crawl_name=['请求线程1号','请求线程2号','请求线程3号']\n",
    "    for name in crawl_name:\n",
    "        #这里用一个循环创建了3个线程\n",
    "        tcrawl=CrawlThread(name,page_queue,data_queue)\n",
    "        g_crawl_list.append(tcrawl)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "#我们通过这个函数来创建解析线程\n",
    "def create_parse_thread(data_queue,fp,lock):\n",
    "    #通过创建paese_name来方便我们了解程序在运行哪个线程\n",
    "    parse_name=['解析线程1号','解析线程2号','解析线程3号']\n",
    "    for name in parse_name:\n",
    "        tparse=ParseThread(name,data_queue,fp,lock)\n",
    "        g_parse_list.append(tparse)\n",
    "\n",
    "        \n",
    "#线程的Class\n",
    "\n",
    "class CrawlThread(threading.Thread):\n",
    "    def __init__(self,name,page_queue,data_queue):\n",
    "        super(CrawlThread,self).__init__()\n",
    "        self.name=name\n",
    "        self.url='http://www.fanjian.net/jianwen-'\n",
    "        self.page_queue=page_queue\n",
    "        self.data_queue=data_queue\n",
    "        self.headers={\n",
    "                        'user_agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/17.17134',\n",
    "                      }\n",
    "    #每个线程都要做什么呢？run函数就是每个线程具体要做些什么\n",
    "    def run(self):\n",
    "        print(self.name)\n",
    "        while 1:\n",
    "            if self.page_queue.empty():\n",
    "                break\n",
    "            url=self.url+str(self.page_queue.get())\n",
    "            r=requests.get(url,headers=self.headers)\n",
    "            print(r.text)\n",
    "            #将获得的数据放到队列当中\n",
    "            self.data_queue.put(r.text)\n",
    "        \n",
    "        \n",
    "        \n",
    "class ParseThread(threading.Thread):\n",
    "    def __init__(self,name,data_queue,fp,lock):\n",
    "        super(ParseThread,self).__init__()\n",
    "        self.name=name\n",
    "        self.data_queue=data_queue\n",
    "        self.fp=fp\n",
    "        self.lock=lock\n",
    "        \n",
    "    def run():\n",
    "        while 1:\n",
    "            if self.data_queue.empty():\n",
    "                break\n",
    "        #从data_queue中获取数据并解析\n",
    "        data=self.data_queue.get()\n",
    "        \n",
    "        #写一个解析函数\n",
    "        self.parse_content(data)\n",
    "    \n",
    "    def parse_content(data):\n",
    "        items=[]\n",
    "        soup=BeautifulSoup(data,'lxml')\n",
    "        a_title=soup.find_all('a',target='_blank')\n",
    "        #获取图片标题\n",
    "        for title in a_title:\n",
    "            my_title=title.text\n",
    "            item={\n",
    "                '标题':my_title\n",
    "            }\n",
    "            items.append(item)\n",
    "        #写入文件，在写入文件之前要先上锁\n",
    "        self.lock.acquire()\n",
    "        self.fp.write(json.dumps(items,ensure_ascii=False)+'\\n')\n",
    "        self.lock.release()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def main():\n",
    "    \n",
    "    #观察网页的url，我们发现不同页数的区别在于url上的参数，所以我们创建一个队列，让这个队列里储存页码数\n",
    "    #让这些页码随机分配到不同的线程中\n",
    "    #创建队列函数\n",
    "    page_queue,data_queue=create_queue()\n",
    "    #我们需要创建请求线程，也要创建解析线程\n",
    "    \n",
    "    fp=open('jian.json','a')\n",
    "    #创建锁\n",
    "    lock=threading.Lock()\n",
    "    \n",
    "    \n",
    "    #先创建请求线程\n",
    "    create_crawl_thread(page_queue,data_queue)\n",
    "    time.sleep(3)\n",
    "    #再创建解析线程\n",
    "    create_parse_thread(data_queue,fp,lock)\n",
    "    for tcrawl in g_crawl_list:\n",
    "        tcrawl.start()\n",
    "        \n",
    "    for tparse in g_parse_list:\n",
    "        tparse.start()\n",
    "    \n",
    "    \n",
    "    for tcrawl in g_crawl_list:\n",
    "        tcrawl.join()\n",
    "    \n",
    "    for tparse in g_parse_list:\n",
    "        tparse.join()\n",
    "    fp.close()\n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
